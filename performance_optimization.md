# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ MOEX-–±–æ—Ç–∞

## üéØ –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –¢–µ–∫—É—â–∏–µ —É–∑–∫–∏–µ –º–µ—Å—Ç–∞:
1. **–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è** - –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å –∫ MOEX API (~150ms)
2. **–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã** - –±–ª–æ–∫–∏—Ä—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
3. **–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤** - –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞—é—Ç—Å—è –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ
4. **–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π** - –Ω–æ–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
5. **–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∞–∫—Ü–∏–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ** - –∫–æ–º–∞–Ω–¥–∞ `/all` –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç 150+ –∞–∫—Ü–∏–π

## üöÄ –†–µ—à–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### 1. Redis –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ

#### data/cache.py
```python
import redis
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import Optional, Any
import logging

logger = logging.getLogger(__name__)

class DataCache:
    """–ö—ç—à –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Ä—ã–Ω–∫–∞"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        try:
            self.redis = redis.from_url(redis_url)
            self.redis.ping()
            logger.info("Redis connected successfully")
        except Exception as e:
            logger.warning(f"Redis connection failed: {e}. Using fallback cache.")
            self.redis = None
            self._fallback_cache = {}
    
    def _make_key(self, ticker: str, interval: str, days: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        return f"market_data:{ticker}:{interval}:{days}"
    
    def get_market_data(self, ticker: str, interval: str, days: int) -> Optional[pd.DataFrame]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞"""
        key = self._make_key(ticker, interval, days)
        
        try:
            if self.redis:
                cached = self.redis.get(key)
                if cached:
                    data = json.loads(cached)
                    df = pd.DataFrame(data['data'])
                    if not df.empty:
                        df.index = pd.to_datetime(data['index'])
                        logger.debug(f"Cache HIT for {key}")
                        return df
            else:
                # Fallback –∫—ç—à –≤ –ø–∞–º—è—Ç–∏
                if key in self._fallback_cache:
                    cached_data, timestamp = self._fallback_cache[key]
                    if datetime.now() - timestamp < timedelta(minutes=5):
                        logger.debug(f"Fallback cache HIT for {key}")
                        return cached_data
        except Exception as e:
            logger.error(f"Cache get error for {key}: {e}")
        
        logger.debug(f"Cache MISS for {key}")
        return None
    
    def set_market_data(self, ticker: str, interval: str, days: int, 
                       df: pd.DataFrame, ttl: int = 300) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à"""
        if df.empty:
            return
            
        key = self._make_key(ticker, interval, days)
        
        try:
            data = {
                'data': df.to_dict('records'),
                'index': df.index.strftime('%Y-%m-%d %H:%M:%S').tolist()
            }
            
            if self.redis:
                self.redis.setex(key, ttl, json.dumps(data, default=str))
                logger.debug(f"Cached data for {key} (TTL: {ttl}s)")
            else:
                # Fallback –∫—ç—à –≤ –ø–∞–º—è—Ç–∏
                self._fallback_cache[key] = (df, datetime.now())
                logger.debug(f"Fallback cached data for {key}")
                
        except Exception as e:
            logger.error(f"Cache set error for {key}: {e}")
    
    def invalidate_ticker(self, ticker: str) -> None:
        """–ò–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–∏–∫–µ—Ä—É"""
        try:
            if self.redis:
                pattern = f"market_data:{ticker}:*"
                keys = self.redis.keys(pattern)
                if keys:
                    self.redis.delete(*keys)
                    logger.info(f"Invalidated {len(keys)} cache entries for {ticker}")
            else:
                # –û—á–∏—Å—Ç–∫–∞ fallback –∫—ç—à–∞
                keys_to_remove = [k for k in self._fallback_cache.keys() 
                                if k.startswith(f"market_data:{ticker}:")]
                for key in keys_to_remove:
                    del self._fallback_cache[key]
                    
        except Exception as e:
            logger.error(f"Cache invalidation error for {ticker}: {e}")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫—ç—à–∞
cache = DataCache()
```

### 2. –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π MOEX –∫–ª–∏–µ–Ω—Ç

#### data/async_moex_client.py
```python
import aiohttp
import asyncio
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging
from data.cache import cache

logger = logging.getLogger(__name__)

class AsyncMOEXClient:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è MOEX API"""
    
    def __init__(self, timeout: int = 10, max_connections: int = 20):
        self.timeout = aiohttp.ClientTimeout(total=timeout)
        self.max_connections = max_connections
        self.session = None
        
    async def __aenter__(self):
        connector = aiohttp.TCPConnector(limit=self.max_connections)
        self.session = aiohttp.ClientSession(
            connector=connector, 
            timeout=self.timeout
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def get_daily_data(self, ticker: str, days: int = 120) -> pd.DataFrame:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–Ω–µ–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_df = cache.get_market_data(ticker, "daily", days)
        if cached_df is not None:
            return cached_df
        
        try:
            till = datetime.today().strftime('%Y-%m-%d')
            from_date = (datetime.today() - timedelta(days=days * 1.5)).strftime('%Y-%m-%d')
            
            url = f"https://iss.moex.com/iss/engines/stock/markets/shares/securities/{ticker}/candles.json"
            params = {
                'interval': 24,
                'from': from_date,
                'till': till
            }
            
            async with self.session.get(url, params=params) as response:
                response.raise_for_status()
                data = await response.json()
                
            df = self._process_candles_data(data, days)
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            cache.set_market_data(ticker, "daily", days, df, ttl=300)
            
            return df
            
        except Exception as e:
            logger.error(f"Error fetching daily data for {ticker}: {e}")
            return pd.DataFrame()
    
    async def get_multiple_daily_data(self, tickers: List[str], days: int = 120) -> Dict[str, pd.DataFrame]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–∏–∫–µ—Ä–æ–≤"""
        tasks = [self.get_daily_data(ticker, days) for ticker in tickers]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        data_dict = {}
        for ticker, result in zip(tickers, results):
            if isinstance(result, Exception):
                logger.error(f"Error getting data for {ticker}: {result}")
                data_dict[ticker] = pd.DataFrame()
            else:
                data_dict[ticker] = result
        
        return data_dict
    
    def _process_candles_data(self, data: dict, limit: int) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π"""
        try:
            candles = data['candles']['data']
            columns = data['candles']['columns']
            
            df = pd.DataFrame(candles, columns=columns)
            
            if df.empty:
                return df
                
            df['begin'] = pd.to_datetime(df['begin'])
            df = df.sort_values('begin')
            df.set_index('begin', inplace=True)
            
            df = df.rename(columns={
                'close': 'close',
                'volume': 'volume', 
                'high': 'high',
                'low': 'low'
            })
            
            df = df[['close', 'volume', 'high', 'low']].dropna()
            return df.tail(limit)
            
        except Exception as e:
            logger.error(f"Error processing candles data: {e}")
            return pd.DataFrame()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
async_moex_client = AsyncMOEXClient()
```

### 3. –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –∏ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞

#### bot/services/background_tasks.py
```python
import asyncio
from datetime import datetime, timedelta
from typing import Set, Dict
import logging
from data.async_moex_client import async_moex_client
from data.cache import cache
from config.sectors import get_all_tickers
from analysis.indicators import analyze_indicators

logger = logging.getLogger(__name__)

class BackgroundTaskManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á"""
    
    def __init__(self):
        self.running = False
        self.preload_task = None
        
    async def start(self):
        """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á"""
        if self.running:
            return
            
        self.running = True
        logger.info("Starting background tasks...")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö
        self.preload_task = asyncio.create_task(self._preload_popular_data())
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É –∫—ç—à–∞
        asyncio.create_task(self._cache_cleanup_loop())
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á"""
        self.running = False
        if self.preload_task:
            self.preload_task.cancel()
        logger.info("Background tasks stopped")
    
    async def _preload_popular_data(self):
        """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∞–∫—Ü–∏–π"""
        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–∏–∫–µ—Ä—ã (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
        popular_tickers = [
            "SBER", "GAZP", "LKOH", "YNDX", "MGNT", "ROSN", 
            "NVTK", "VTBR", "ALRS", "MTSS", "MOEX", "PIKK"
        ]
        
        while self.running:
            try:
                logger.info("Preloading popular stocks data...")
                
                async with async_moex_client as client:
                    # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–Ω–µ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    await client.get_multiple_daily_data(popular_tickers, days=120)
                    
                    # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º 4H –¥–∞–Ω–Ω—ã–µ
                    tasks = [client.get_4h_data(ticker, days=25) for ticker in popular_tickers]
                    await asyncio.gather(*tasks, return_exceptions=True)
                
                logger.info(f"Preloaded data for {len(popular_tickers)} popular stocks")
                
                # –ñ–¥–µ–º 10 –º–∏–Ω—É—Ç –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                await asyncio.sleep(600)
                
            except Exception as e:
                logger.error(f"Error in preload task: {e}")
                await asyncio.sleep(60)  # –ñ–¥–µ–º –º–∏–Ω—É—Ç—É –ø—Ä–∏ –æ—à–∏–±–∫–µ
    
    async def _cache_cleanup_loop(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        while self.running:
            try:
                # –û—á–∏—â–∞–µ–º –∫—ç—à –∫–∞–∂–¥—ã–µ 30 –º–∏–Ω—É—Ç
                await asyncio.sleep(1800)
                
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                logger.debug("Cache cleanup completed")
                
            except Exception as e:
                logger.error(f"Error in cache cleanup: {e}")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä
background_manager = BackgroundTaskManager()
```

### 4. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥

#### bot/handlers/optimized_analysis.py
```python
import asyncio
from telegram import Update
from telegram.ext import ContextTypes
from data.async_moex_client import async_moex_client
from analysis.indicators import analyze_indicators
from config.sectors import get_all_tickers
from bot.utils.decorators import rate_limit, handle_errors, log_command
import logging

logger = logging.getLogger(__name__)

@rate_limit
@handle_errors
@log_command
async def rsi_top_optimized(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ /rsi_top —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    
    message = await update.message.reply_text("üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é RSI –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ü–∏–π... (–±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º)")
    
    try:
        tickers = get_all_tickers()
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        batch_size = 20
        batches = [tickers[i:i + batch_size] for i in range(0, len(tickers), batch_size)]
        
        overbought = []
        oversold = []
        
        async with async_moex_client as client:
            for batch in batches:
                # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞—Ç—á–∞
                data_dict = await client.get_multiple_daily_data(batch, days=30)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                for ticker, df in data_dict.items():
                    if df.empty:
                        continue
                        
                    try:
                        df = analyze_indicators(df)
                        rsi = df['RSI'].iloc[-1]
                        
                        if pd.notna(rsi):
                            if rsi > 70:
                                overbought.append((ticker, rsi))
                            elif rsi < 30:
                                oversold.append((ticker, rsi))
                    except Exception as e:
                        logger.warning(f"Error processing {ticker}: {e}")
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                await asyncio.sleep(0.1)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        overbought.sort(key=lambda x: x[1], reverse=True)
        oversold.sort(key=lambda x: x[1])
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = "üìä **RSI –ê–Ω–∞–ª–∏–∑** (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)\n\n"
        
        if overbought:
            response += "üî¥ **–ü–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω—ã–µ –∞–∫—Ü–∏–∏ (RSI > 70):**\n"
            for ticker, rsi in overbought[:10]:
                response += f"‚Ä¢ {ticker}: {rsi:.0f}\n"
        
        if oversold:
            response += "\nüü¢ **–ü–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω—ã–µ –∞–∫—Ü–∏–∏ (RSI < 30):**\n"
            for ticker, rsi in oversold[:10]:
                response += f"‚Ä¢ {ticker}: {rsi:.0f}\n"
        
        if not overbought and not oversold:
            response += "‚ÑπÔ∏è –ù–µ—Ç –∞–∫—Ü–∏–π –≤ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–æ–Ω–∞—Ö RSI"
        
        await message.edit_text(response)
        
    except Exception as e:
        logger.error(f"Error in optimized rsi_top: {e}")
        await message.edit_text("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ RSI")

@rate_limit
@handle_errors  
@log_command
async def market_scan_fast(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ë—ã—Å—Ç—Ä–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä—ã–Ω–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞"""
    
    await update.message.reply_text("üöÄ –ë—ã—Å—Ç—Ä–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä—ã–Ω–∫–∞...")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞
    popular_tickers = ["SBER", "GAZP", "LKOH", "YDEX", "MGNT", "ROSN", "NVTK", "VTBR"]
    
    async with async_moex_client as client:
        data_dict = await client.get_multiple_daily_data(popular_tickers, days=50)
    
    results = []
    for ticker, df in data_dict.items():
        if not df.empty:
            df = analyze_indicators(df)
            
            # –ê–Ω–∞–ª–∏–∑ —Å–∏–≥–Ω–∞–ª–æ–≤
            current_price = df['close'].iloc[-1]
            ema20 = df['EMA20'].iloc[-1] if 'EMA20' in df.columns else None
            ema50 = df['EMA50'].iloc[-1] if 'EMA50' in df.columns else None
            rsi = df['RSI'].iloc[-1] if 'RSI' in df.columns else None
            
            signal = "üìä"
            if ema20 and ema50 and current_price > ema20 > ema50:
                signal = "üü¢"
            elif ema20 and ema50 and current_price < ema20 < ema50:
                signal = "üî¥"
            
            results.append({
                'ticker': ticker,
                'price': current_price,
                'rsi': rsi,
                'signal': signal
            })
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    response = "üöÄ **–ë—ã—Å—Ç—Ä–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä—ã–Ω–∫–∞**\n\n"
    for item in results:
        rsi_text = f"RSI: {item['rsi']:.0f}" if item['rsi'] else "RSI: N/A"
        response += f"{item['signal']} {item['ticker']}: {item['price']:.2f} | {rsi_text}\n"
    
    await update.message.reply_text(response)
```

### 5. –ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤

#### data/connection_pool.py
```python
import aiohttp
import asyncio
from contextlib import asynccontextmanager
from typing import Optional
import logging

logger = logging.getLogger(__name__)

class ConnectionPool:
    """–ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self, max_connections: int = 100, max_connections_per_host: int = 20):
        self.max_connections = max_connections
        self.max_connections_per_host = max_connections_per_host
        self._session: Optional[aiohttp.ClientSession] = None
        self._lock = asyncio.Lock()
    
    async def get_session(self) -> aiohttp.ClientSession:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏ (—Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)"""
        if self._session is None or self._session.closed:
            async with self._lock:
                if self._session is None or self._session.closed:
                    connector = aiohttp.TCPConnector(
                        limit=self.max_connections,
                        limit_per_host=self.max_connections_per_host,
                        keepalive_timeout=30,
                        enable_cleanup_closed=True
                    )
                    
                    timeout = aiohttp.ClientTimeout(total=30, connect=10)
                    
                    self._session = aiohttp.ClientSession(
                        connector=connector,
                        timeout=timeout,
                        headers={'User-Agent': 'MOEX-Bot/1.0'}
                    )
                    
                    logger.info("Created new HTTP session with connection pool")
        
        return self._session
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        if self._session and not self._session.closed:
            await self._session.close()
            logger.info("Connection pool closed")
    
    @asynccontextmanager
    async def request(self, method: str, url: str, **kwargs):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–æ–≤"""
        session = await self.get_session()
        async with session.request(method, url, **kwargs) as response:
            yield response

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
connection_pool = ConnectionPool()
```

### 6. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

#### bot/utils/performance.py
```python
import time
import functools
import asyncio
from typing import Dict, List
import logging
from dataclasses import dataclass, field
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    total_requests: int = 0
    total_time: float = 0.0
    errors: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    response_times: List[float] = field(default_factory=list)
    
    @property
    def average_response_time(self) -> float:
        return self.total_time / self.total_requests if self.total_requests > 0 else 0.0
    
    @property
    def cache_hit_rate(self) -> float:
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0

class PerformanceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.metrics: Dict[str, PerformanceMetrics] = {}
        self.start_time = datetime.now()
    
    def record_request(self, endpoint: str, duration: float, error: bool = False, 
                      cache_hit: bool = False):
        """–ó–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–ø—Ä–æ—Å–∞"""
        if endpoint not in self.metrics:
            self.metrics[endpoint] = PerformanceMetrics()
        
        metrics = self.metrics[endpoint]
        metrics.total_requests += 1
        metrics.total_time += duration
        metrics.response_times.append(duration)
        
        if error:
            metrics.errors += 1
        
        if cache_hit:
            metrics.cache_hits += 1
        else:
            metrics.cache_misses += 1
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–ø–∏—Å–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞
        if len(metrics.response_times) > 1000:
            metrics.response_times = metrics.response_times[-500:]
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        stats = {
            'uptime': str(datetime.now() - self.start_time),
            'endpoints': {}
        }
        
        for endpoint, metrics in self.metrics.items():
            stats['endpoints'][endpoint] = {
                'total_requests': metrics.total_requests,
                'average_response_time': f"{metrics.average_response_time:.3f}s",
                'errors': metrics.errors,
                'error_rate': f"{metrics.errors / metrics.total_requests * 100:.1f}%" if metrics.total_requests > 0 else "0%",
                'cache_hit_rate': f"{metrics.cache_hit_rate * 100:.1f}%"
            }
        
        return stats

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä
performance_monitor = PerformanceMonitor()

def monitor_performance(endpoint: str):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    def decorator(func):
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            error = False
            cache_hit = False
            
            try:
                result = await func(*args, **kwargs)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ cache hit (–º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É)
                cache_hit = getattr(result, '_from_cache', False)
                return result
            except Exception as e:
                error = True
                raise
            finally:
                duration = time.time() - start_time
                performance_monitor.record_request(endpoint, duration, error, cache_hit)
        
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            error = False
            
            try:
                return func(*args, **kwargs)
            except Exception as e:
                error = True
                raise
            finally:
                duration = time.time() - start_time
                performance_monitor.record_request(endpoint, duration, error)
        
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    
    return decorator
```

### 7. –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π main.py —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏

#### main_optimized.py
```python
#!/usr/bin/env python3
"""
Optimized MOEX Telegram Bot
"""

import asyncio
import logging
from telegram.ext import ApplicationBuilder, CommandHandler
from config.settings import settings
from bot.handlers.basic import start
from bot.handlers.optimized_analysis import rsi_top_optimized, market_scan_fast
from bot.services.background_tasks import background_manager
from data.connection_pool import connection_pool

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=getattr(logging, settings.log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def setup_background_tasks():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á"""
    await background_manager.start()

async def cleanup():
    """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
    await background_manager.stop()
    await connection_pool.close()

def setup_handlers(app):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤"""
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("rsi_top", rsi_top_optimized))
    app.add_handler(CommandHandler("scan_fast", market_scan_fast))
    
    logger.info("Optimized handlers registered")

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        # –ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á
        await setup_background_tasks()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        app = ApplicationBuilder().token(settings.telegram_token).build()
        setup_handlers(app)
        
        logger.info("Starting optimized bot...")
        
        # –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ webhook
        await app.initialize()
        await app.start()
        
        # Webhook –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
        await app.bot.set_webhook(
            url=f"{settings.webhook_url}/{settings.telegram_token}"
        )
        
        # –ü–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ —Ä–∞–±–æ—Ç—ã
        await app.updater.start_webhook(
            listen="0.0.0.0",
            port=8080,
            url_path=settings.telegram_token
        )
        
        logger.info("Bot is running...")
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        await asyncio.Event().wait()
        
    except KeyboardInterrupt:
        logger.info("Shutting down...")
    finally:
        await cleanup()

if __name__ == '__main__':
    asyncio.run(main())
```

## üìä –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –î–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
- –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: **10-30 —Å–µ–∫—É–Ω–¥**
- –ó–∞–ø—Ä–æ—Å—ã –∫ API: **150+ –Ω–∞ –∫–æ–º–∞–Ω–¥—É**
- –ü–∞–º—è—Ç—å: **–≤—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ**
- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏: **–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –ø—Ä–∏ –Ω–∞–≥—Ä—É–∑–∫–µ**

### –ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
- –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: **1-5 —Å–µ–∫—É–Ω–¥** ‚ö°
- –ó–∞–ø—Ä–æ—Å—ã –∫ API: **0-50 –Ω–∞ –∫–æ–º–∞–Ω–¥—É** (–∫—ç—à)
- –ü–∞–º—è—Ç—å: **–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ**
- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏: **–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞**

### –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- **Cache Hit Rate: 70-90%** - –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫—ç—à–∞
- **Parallel Processing: 10-20x** –±—ã—Å—Ç—Ä–µ–µ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
- **Background Preloading** - –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤—Å–µ–≥–¥–∞ –≥–æ—Ç–æ–≤—ã
- **Connection Pooling** - —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
- **Async/Await** - –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

## üöÄ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### 8. Database –≤–º–µ—Å—Ç–æ —Ñ–∞–π–ª–æ–≤
```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ PostgreSQL –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
# –ò–Ω–¥–µ–∫—Å—ã –ø–æ —Ç–∏–∫–µ—Ä–∞–º –∏ –¥–∞—Ç–∞–º
# –ú–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
```

### 9. CDN –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
```python
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ S3/CloudFlare
# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
```

### 10. Message Queue
```python
# Redis/RabbitMQ –¥–ª—è —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
# –û—á–µ—Ä–µ–¥–∏ –¥–ª—è —Ç—è–∂–µ–ª—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
```

–≠—Ç–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–¥–µ–ª–∞—é—Ç –≤–∞—à –±–æ—Ç **–≤ 10-20 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ** –∏ –≥–æ—Ç–æ–≤—ã–º –∫ –≤—ã—Å–æ–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–µ! üéØ